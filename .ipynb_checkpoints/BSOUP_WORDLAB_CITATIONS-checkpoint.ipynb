{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from requests import get\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from unidecode import unidecode\n",
    "whitelist = set('µÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿ')\n",
    "import time\n",
    "\n",
    "# Read input file\n",
    "file_to_open = \"Urbanisation DOI list_edit.csv\"\n",
    "\n",
    "df = pd.read_csv(file_to_open)\n",
    "\n",
    "# Create header for output file\n",
    "list1=['metrics_page'\n",
    "       , 'Title'\n",
    "       , 'Keywords'\n",
    "       , 'Author1'\n",
    "       , 'Author2'\n",
    "       , 'Author3'\n",
    "       , 'Author4'\n",
    "       , 'Author5'\n",
    "       , 'Author6'\n",
    "       , 'Author7'\n",
    "       , 'Author8'\n",
    "       , 'Author9'\n",
    "       , 'Author10'\n",
    "       , 'Corresponding_author'\n",
    "       , 'Corresponding_author_email'\n",
    "       , 'Number_of_views_downloads'\n",
    "       , 'Number_of_citations'\n",
    "       , 'Citation1'\n",
    "       , 'Citation2'\n",
    "       , 'Citation3'\n",
    "       , 'Citation4'\n",
    "       , 'Citation5'\n",
    "       , 'Citation6'\n",
    "       , 'Citation7'\n",
    "       , 'Citation8'\n",
    "       , 'Citation9'\n",
    "       , 'Citation10'\n",
    "       , 'Citation11'\n",
    "       , 'Citation12'\n",
    "       , 'Citation13'\n",
    "       , 'Citation14'\n",
    "       , 'Citation15'\n",
    "       , 'Citation16'\n",
    "       , 'Citation17'\n",
    "       , 'Citation18'\n",
    "       , 'Citation19'\n",
    "       , 'Citation20'\n",
    "       , 'Citation21'\n",
    "       , 'Citation22'\n",
    "       , 'Citation23'\n",
    "       , 'Citation24'\n",
    "       , 'Citation25'\n",
    "       , 'Citation26'\n",
    "       , 'Citation27'\n",
    "       , 'Citation28'\n",
    "       , 'Citation29'\n",
    "       , 'Citation30'\n",
    "       , 'Citation31'\n",
    "       , 'Citation32'\n",
    "       , 'Citation33'\n",
    "       , 'Citation34'\n",
    "       , 'Citation35'\n",
    "       , 'Citation36'\n",
    "       , 'Citation37'\n",
    "       , 'Citation38'\n",
    "       , 'Citation39'\n",
    "       , 'Citation40'\n",
    "       ]\n",
    "with open(\"urbanisation_author_and_citation_detail.csv\", \"a\", encoding=\"utf-8\", newline='') as fp:\n",
    "    wr = csv.writer(fp, dialect='excel')\n",
    "    wr.writerow(list1)\n",
    "\n",
    "    \n",
    "# Process input file\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    title = ' '\n",
    "    keywords = ' '\n",
    "    corresp_author = ' '\n",
    "    corrsp_email = ' '\n",
    "    views = '0'\n",
    "    citations = '0'\n",
    "    citation_details = []\n",
    "    \n",
    "    print(row['DOI Link'])\n",
    "    page=row['DOI Link']\n",
    "    \n",
    "    # Get landing page \n",
    "\n",
    "#    options = Options()\n",
    "#    options.headless = True\n",
    "#    driver = webdriver.Firefox(options=options,\n",
    "    driver = webdriver.Firefox(\n",
    "        executable_path=r'C:\\Users\\vlewis\\AppData\\Local\\Temp\\Temp1_geckodriver-v0.30.0-win64.zip\\geckodriver.exe')\n",
    "\n",
    "    driver.get(page)\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")    \n",
    "    \n",
    "    # Get keywords\n",
    "    keywords = soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "    \n",
    "    if keywords:\n",
    "        keywords = keywords[\"content\"]\n",
    "    print(keywords)\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    # get metrics page\n",
    "    page=page.replace(\"https://doi.org\",\"https://journals.sagepub.com/doi/metrics\")\n",
    "    print(page)\n",
    "    \n",
    "#    options = Options()\n",
    "#    options.headless = True\n",
    "#    driver = webdriver.Firefox(options=options,\n",
    "    driver = webdriver.Firefox(\n",
    "        executable_path=r'C:\\Users\\vlewis\\AppData\\Local\\Temp\\Temp1_geckodriver-v0.30.0-win64.zip\\geckodriver.exe')\n",
    "\n",
    "    driver.get(page)\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")   \n",
    "    \n",
    "    # title\n",
    "    title = soup.find(\"div\", attrs={\"class\": \"publicationContentTitle\"})\n",
    "    # print(title)\n",
    "    if title:\n",
    "        title = title.text.replace('\\n', ' ').strip()\n",
    "        title = ''.join(c if c in whitelist else unidecode(c) for c in title)\n",
    "    print(title)\n",
    "    \n",
    "    # author\n",
    "    dir_author_name = soup.find(\"div\", attrs={\"class\": \"publicationContentAuthors\"})\n",
    "    author_name = [None] * 10\n",
    "    artTags = dir_author_name.find_all(\"div\", {\"class\": \"authorLayer\"})\n",
    "    i = 0\n",
    "    for tag11 in artTags:\n",
    "        tag11 = tag11.text.replace('\\n', ' ').strip()\n",
    "        tag11 = tag11.replace(u'\\xa0', u' ')\n",
    "        tag11 = ''.join(c if c in whitelist else unidecode(c) for c in tag11)\n",
    "        author_name[i] = tag11.replace('See all articles by this author Search Google Scholar  for this author', ' ')\n",
    "        i = i + 1\n",
    "    print(author_name)\n",
    "    \n",
    "    # corresponding author\n",
    "    corresp_author = soup.find(\"div\", attrs={\"class\": \"artice-notes\"})\n",
    "    if corresp_author:\n",
    "        corresp_author = corresp_author.text.replace('Corresponding Author:', ' ').strip()\n",
    "        corresp_author = corresp_author.replace('Corresponding author:', ' ').strip()\n",
    "        corresp_author = corresp_author.split(\"E-mail\",1)[0]\n",
    "        corresp_author = ''.join(c if c in whitelist else unidecode(c) for c in corresp_author)\n",
    "        print(corresp_author)\n",
    "    \n",
    "    # corresponding author email  \n",
    "    corrsp_email = ' '\n",
    "    def decodeEmail(e):\n",
    "        de = \"\"\n",
    "        k = int(e[:2], 16)\n",
    "\n",
    "        for i in range(2, len(e)-1, 2):\n",
    "            de += chr(int(e[i:i+2], 16)^k)\n",
    "        return de\n",
    "    \n",
    "    prot_email = soup.find(\"span\", {\"class\": \"__cf_email__\"})\n",
    "    if prot_email:\n",
    "        e = prot_email[\"data-cfemail\"]\n",
    "        corrsp_email = decodeEmail(e)\n",
    "        print(corrsp_email)\n",
    "        \n",
    "    # number of views\n",
    "    views = soup.find(\"div\", attrs={\"class\": \"widget literatumContentItemDownloadCount none articleUsage widget-none widget-compact-horizontal\"})\n",
    "    \n",
    "    #dir_views\n",
    "    views = views.text.replace('\\n', ' ').strip().split(\":\",1)[1]\n",
    "    print(views)\n",
    "    \n",
    "    # citations\n",
    "    dir_citations = soup.find(\"div\", attrs={\"class\": \"widget literatumContentItemServiceCitaionLinks none widget-none widget-compact-all\"})\n",
    "\n",
    "    for table in dir_citations.find_all(\"table\"):\n",
    "        t_value = []\n",
    "        for td in table.find_all(\"td\"):\n",
    "            t_value.append(td.text.replace('\\n', ' ').strip())\n",
    "    citations = t_value[1]\n",
    "    print(citations)\n",
    "\n",
    "    if citations != '0':\n",
    "        print('found citations')\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    # access citations\n",
    "    page=row['DOI Link']\n",
    "    page=page.replace(\"https://doi.org\",\"https://journals.sagepub.com/doi/citedby\")\n",
    "    print(page)\n",
    "    \n",
    "#    options = Options()\n",
    "#    options.headless = True\n",
    "#    driver = webdriver.Firefox(options=options,\n",
    "    driver = webdriver.Firefox(\n",
    "        executable_path=r'C:\\Users\\vlewis\\AppData\\Local\\Temp\\Temp1_geckodriver-v0.30.0-win64.zip\\geckodriver.exe')\n",
    "\n",
    "    driver.get(page)\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"lxml\")   \n",
    "    \n",
    "    # citations details\n",
    "    citation_details = [None] * 40\n",
    "    i = 0\n",
    "    if (citations != '0'and page != 'https://journals.sagepub.com/doi/citedby/10.1177/2455747119863891'):\n",
    "        count = -1\n",
    "        for tag in soup.find(\"div\", attrs={\"class\": \"citedBySection\"}):\n",
    "            tag1 = ''\n",
    "            tag2 = ''\n",
    "            tag3 = ''\n",
    "            tag4 = ''\n",
    "            tag5 = ''\n",
    "            print(count)\n",
    "    #    print('citedby')\n",
    "            artTags = tag.find_all(\"div\", {\"class\": \"articleTitle\"})\n",
    "            for tag1 in artTags:\n",
    "                print('articletitle')\n",
    "                print(tag1.text)\n",
    "                tag1 = tag1.text\n",
    "            authTags = tag.find_all(\"div\", attrs={\"class\": \"authors\"})\n",
    "            for tag2 in authTags:\n",
    "    #        print('authors')\n",
    "                print(tag2.text)\n",
    "                tag2 = tag2.text\n",
    "            journalTags = tag.find_all(\"div\", attrs={\"class\": \"journalTitle\"})\n",
    "            for tag3 in journalTags:\n",
    "    #        print('journaltitle')\n",
    "                print(tag3.text)  \n",
    "                tag3 = tag3.text\n",
    "            pubdateTags = tag.find_all(\"div\", attrs={\"class\": \"pubDate\"})\n",
    "            for tag4 in pubdateTags:\n",
    "    #        print('pubdate')\n",
    "                print(tag4.text)   \n",
    "                tag4 = tag4.text\n",
    "            linkTags = tag.find_all(\"a\", attrs={\"class\": \"ref\"})\n",
    "            for tag5 in linkTags:\n",
    "    #        print('link')\n",
    "                print(tag5[\"href\"])  \n",
    "                tag5 = tag5[\"href\"]\n",
    "            count = count + 1\n",
    "#            print(tag1)\n",
    "            if count > 1:\n",
    "                citation_tags = str(tag1) +'\\n'+str(tag2)+'\\n' +str(tag3)+'\\n' +str(tag4)+'\\n' +str(tag5)\n",
    "#                citation_tags = str(tag1) +'\\n'+str(tag2)+'\\n' +str(tag3)+'\\n' +str(tag4)+'\\n' +\"=HYPERLINK(\" +'\"'+ tag5+'\"' +\",\"+'\"' + tag5+'\"' +\")\"\n",
    "                citation_tags = ''.join(c if c in whitelist else unidecode(c) for c in citation_tags)\n",
    "                citation_details[i] = citation_tags    \n",
    "                i = i + 1\n",
    "#        \n",
    "    driver.close()\n",
    "    \n",
    "#   write to output\n",
    "    print('write to output')\n",
    "    list1=[\"=HYPERLINK(\" +'\"'+ page+'\"' +\",\"+'\"' + page+'\"' +\")\"\n",
    "       , title\n",
    "       , keywords\n",
    "       , author_name[0]\n",
    "       , author_name[1]\n",
    "       , author_name[2]\n",
    "       , author_name[3]\n",
    "       , author_name[4]\n",
    "       , author_name[5]\n",
    "       , author_name[6]\n",
    "       , author_name[7]\n",
    "       , author_name[8]\n",
    "       , author_name[9]\n",
    "       , corresp_author\n",
    "       , corrsp_email\n",
    "       , views\n",
    "       , citations\n",
    "       , citation_details[0]\n",
    "       , citation_details[1]   \n",
    "       , citation_details[2]   \n",
    "       , citation_details[3]   \n",
    "       , citation_details[4]   \n",
    "       , citation_details[5]   \n",
    "       , citation_details[6]   \n",
    "       , citation_details[7]   \n",
    "       , citation_details[8]   \n",
    "       , citation_details[9]   \n",
    "       , citation_details[10]\n",
    "       , citation_details[11]   \n",
    "       , citation_details[12]   \n",
    "       , citation_details[13]   \n",
    "       , citation_details[14]   \n",
    "       , citation_details[15]   \n",
    "       , citation_details[16]   \n",
    "       , citation_details[17]   \n",
    "       , citation_details[18]   \n",
    "       , citation_details[19]  \n",
    "       , citation_details[20]\n",
    "       , citation_details[21]   \n",
    "       , citation_details[22]   \n",
    "       , citation_details[23]   \n",
    "       , citation_details[24]   \n",
    "       , citation_details[25]   \n",
    "       , citation_details[26]   \n",
    "       , citation_details[27]   \n",
    "       , citation_details[28]   \n",
    "       , citation_details[29]  \n",
    "       , citation_details[30]\n",
    "       , citation_details[31]   \n",
    "       , citation_details[32]   \n",
    "       , citation_details[33]   \n",
    "       , citation_details[34]   \n",
    "       , citation_details[35]   \n",
    "       , citation_details[36]   \n",
    "       , citation_details[37]   \n",
    "       , citation_details[38]   \n",
    "       , citation_details[39]  \n",
    "          ]\n",
    "    \n",
    "    print(list1)\n",
    "    with open(\"urbanisation_author_and_citation_detail.csv\", \"a\", encoding=\"utf-8\", newline='') as fp:\n",
    "        wr = csv.writer(fp, dialect='excel')\n",
    "        wr.writerow(list1)\n",
    "        \n",
    "driver.quit()\n",
    "      \n",
    "################## End of Code ###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
